In pursuit of an accurate and dependable image-based fruit detection system to support advanced agricultural tasks such as yield mapping and robotic harvesting, the authors of the **ACFR** dataset introduce the application of an object detection framework, Faster R-CNN. This framework is employed within the context of orchard fruit detection, encompassing various fruit types, including mangoes, almonds, and apples.

In agricultural automation, vision-based fruit detection stands as a pivotal element. It provides the foundation for in-field automation, enabling tasks such as yield estimation, mapping, and ultimately contributing to efficient resource utilization and enhanced returns per unit area and time. Additionally, precise fruit localization is an indispensable aspect of automated robotic harvesting systems, offering relief from one of the most labor-intensive challenges in orchard management.

The dataset under evaluation in this study encompasses three distinct fruit varieties: _apple_, _almond_, and _mango_. These data were collected during daylight hours at orchards located in Victoria and Queensland, Australia. For data collection, apple and mango data were acquired through sensors integrated into a versatile research ground vehicle developed at the Australian Centre for Field Robotics. This vehicle traversed different orchard rows to collect tree image data. Apple trees, which are trellised, allowed the ground vehicle to maintain close proximity to the fruit. In contrast, the greater distance between mangoes and the ground vehicle was compensated for with a higher resolution sensor. In the mango orchard, external strobe lighting with a brief exposure time (approximately 70 Âµs) was employed to mitigate variable illumination artifacts. Almond trees, characterized by larger canopies and the potential to host between 1000 to 10,000 almonds, necessitated high-resolution imagery, which was acquired using a handheld DSLR camera. The images captured for each orchard encompassed entire trees, aligning with the primary research goal of efficient yield estimation and mapping. The fruit detection aspect presented in this paper constitutes a pivotal component of this overarching project.

| Fruit   | Image Info                                                                                                    | Annotation Info                                          | Notes                                                                   |
| ------- | ------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- | ----------------------------------------------------------------------- |
| Apples  | 1120 Images<br/>PNG Image Data<br/>308 x 202, 8-bit/color RGB<br/>Sensor: PointGrey Ladybug3                  | Circle Annotation (x,y,radius)<br/>Pixel-wise annotation | Collected at Warburton, Australia<br/>Apple varieties: Pink Lady, Kanzi |
| Mangoes | 1964 Images<br/>PNG Image Data<br/>500 x 500, 16-bit/color RGB<br/>Sensor: Prosilica GT3300c<br/>Strobes Used | Rectangle Annotation (x,y,dx,dy)                         | Collected at Bundaberg, Australia<br/>Mango varieties: Calypso          |
| Almonds | 620 Images<br/>PNG Image Data<br/>308 x 202, 8-bit/color RGB<br/>Sensor: Canon EOS60D                         | Rectangle Annotation (x,y,dx,dy)                         | Collected at Mildura, Australia<br/>Almond variety: Nonpareil           |

The tree image data exhibited variations in size, ranging from 2 to 17 megapixels, with each image containing approximately 100 fruit for apples and mangoes, and over 1000 fruit for almonds. However, hardware constraints limited the usage of large images. For instance, a 0.25-megapixel image required around 2.5 gigabytes of GPU memory with the VGG16 network. Additionally, labeling a substantial number of small objects within large images proved to be a perceptually challenging task. To address these challenges, the authors mitigated them by randomly sampling smaller sub-image patches from the pool of larger images obtained across the farm. This approach yielded smaller images, akin in size to the PASCAL-VOC dataset used with Faster R-CNN, featuring lower fruit counts while effectively covering data variability across the farm.

The ground truth fruit annotations for almonds and mangoes were established through rectangular annotations, while circular annotations were better suited for apples. However, since Faster R-CNN operates on bounding box predictions, circular annotations were initially converted to rectangles with equal width and height. <i>Note, that datasetninja converted circle bounding boxes to the rectangular ones.</i> In practice, labeling apples and mangoes was relatively more straightforward compared to almonds, primarily due to the size and contrast of the fruit in relation to the surrounding foliage and the canopy's complexity. To facilitate fruit-background differentiation in shadowed regions of the image, the annotation software incorporated sliders for contrast and brightness adjustments.

Finally, each fruit's labeled dataset was subdivided into training, validation, and testing sets. This division was carefully structured to ensure that each set comprised data collected from different sections of the orchard block, minimizing potential biases in the results. Images in the training set lacking any fruit presence were excluded from consideration.
